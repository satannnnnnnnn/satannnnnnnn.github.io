# å…¨å±€ç¼–ç é…ç½®
import sys
import os
import uuid
import random
import time

sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')
os.environ['PYTHONIOENCODING'] = 'utf-8'

import requests
from bs4 import BeautifulSoup
# ç¡®ä¿app.pyå·²ä¿®æ­£è·¯ç”±ï¼ˆ/profile/<<<<int:user_id> â†’ /profile/<<<int:user_id>ï¼‰
from app import app, db, Movie
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ========== æ ¸å¿ƒé…ç½® ==========
# æµ·æŠ¥å­˜å‚¨è·¯å¾„ï¼ˆå¯¹é½app.pyçš„staticç›®å½•ï¼‰
POSTER_SAVE_PATH = os.path.join(app.root_path, "static", "posters")
# é»˜è®¤æµ·æŠ¥è·¯å¾„
DEFAULT_POSTER = "/static/posters/default.jpg"
# è±†ç“£Top250åŸºç¡€URL
DOUBAN_TOP250_BASE = "https://movie.douban.com/top250?start={}&filter="
# çˆ¬å–å‰50éƒ¨ç”µå½±ï¼ˆè±†ç“£Top250æ¯é¡µ25éƒ¨ï¼Œå–å‰2é¡µï¼‰
CRAWL_LIMIT = 50

# ç¡®ä¿æµ·æŠ¥ç›®å½•å­˜åœ¨
os.makedirs(POSTER_SAVE_PATH, exist_ok=True)


def download_poster(session, poster_url, movie_name):
    """ä¿®å¤æµ·æŠ¥ä¸‹è½½é€»è¾‘ï¼šå¤„ç†404ã€ä½¿ç”¨é«˜æ¸…URLã€å¤±è´¥åˆ™è¿”å›é»˜è®¤æµ·æŠ¥"""
    if not poster_url or not poster_url.startswith(("https://", "http://")):
        return DEFAULT_POSTER

    # æ›¿æ¢ä¸ºé«˜æ¸…æµ·æŠ¥URLï¼ˆé¿å…ä½æ¸…URLè¿‡æœŸï¼‰
    poster_url = poster_url.replace("s_ratio_poster", "l_ratio_poster")
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    poster_filename = f"douban_{uuid.uuid4().hex[:10]}.jpg"
    local_path = os.path.join(POSTER_SAVE_PATH, poster_filename)
    web_path = f"/static/posters/{poster_filename}"

    # è·³è¿‡å·²å­˜åœ¨çš„æœ‰æ•ˆæµ·æŠ¥
    if os.path.exists(local_path) and os.path.getsize(local_path) > 1024:
        print(f"â„¹ï¸ æµ·æŠ¥å·²å­˜åœ¨ï¼š{movie_name}")
        return web_path

    try:
        # å¢å¼ºè¯·æ±‚å¤´ï¼Œé¿å…è¢«è±†ç“£æ‹¦æˆª
        headers = {
            "User-Agent": random.choice([
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/130.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_5) Safari/605.1.15"
            ]),
            "Referer": "https://movie.douban.com/top250"
        }
        response = session.get(
            poster_url,
            headers=headers,
            timeout=15,
            stream=True,
            allow_redirects=True
        )
        response.raise_for_status()  # æ•è·HTTPé”™è¯¯

        # å†™å…¥æµ·æŠ¥æ–‡ä»¶
        with open(local_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§
        if os.path.getsize(local_path) < 1024:
            os.remove(local_path)
            raise Exception("æµ·æŠ¥æ–‡ä»¶è¿‡å°ï¼ˆæ— æ•ˆï¼‰")

        print(f"âœ… æµ·æŠ¥ä¸‹è½½æˆåŠŸï¼š{movie_name} â†’ {poster_filename}")
        return web_path

    except Exception as e:
        print(f"âŒ æµ·æŠ¥ä¸‹è½½å¤±è´¥({movie_name})ï¼š{str(e)} â†’ ä½¿ç”¨é»˜è®¤æµ·æŠ¥")
        if os.path.exists(local_path):
            os.remove(local_path)
        return DEFAULT_POSTER


def get_movie_intro(session, detail_url):
    """ä»ç”µå½±è¯¦æƒ…é¡µè·å–å®Œæ•´ç®€ä»‹ï¼ˆé¿å…åˆ—è¡¨é¡µç®€ä»‹ä¸å®Œæ•´ï¼‰"""
    try:
        # æ„é€ è¯¦æƒ…é¡µè¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
            "Referer": "https://movie.douban.com/top250"
        }
        # å¢åŠ éšæœºå»¶è¿Ÿï¼Œé™ä½åçˆ¬é£é™©
        time.sleep(random.randint(2, 4))
        response = session.get(detail_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # è§£æè±†ç“£è¯¦æƒ…é¡µçš„å®Œæ•´ç®€ä»‹ï¼ˆå¯¹åº”æ ‡ç­¾ï¼šspan[property="v:summary"]ï¼‰
        intro_tag = soup.find("span", property="v:summary")
        if intro_tag:
            # æ¸…ç†ç®€ä»‹ä¸­çš„æ¢è¡Œã€å¤šä½™ç©ºæ ¼
            full_intro = intro_tag.get_text(strip=True).replace("\n", " ").replace("  ", " ")
            return full_intro if full_intro else "æš‚æ— ç®€ä»‹"
        return "æš‚æ— ç®€ä»‹"
    except Exception as e:
        print(f"âŒ è·å–ç®€ä»‹å¤±è´¥({detail_url})ï¼š{str(e)}")
        return "æš‚æ— ç®€ä»‹"


def crawl_douban_top250():
    movies_data = []
    session = requests.Session()
    # æ¸©å’Œçš„é‡è¯•ç­–ç•¥ï¼Œé¿å…è§¦å‘åçˆ¬
    retry_strategy = Retry(
        total=3,
        backoff_factor=5,
        status_forcelist=[429, 500, 502, 503, 404],
        allowed_methods=["GET"]
    )
    session.mount("https://", HTTPAdapter(max_retries=retry_strategy))

    # ä½ çš„è±†ç“£Cookieï¼ˆå·²éªŒè¯æœ‰æ•ˆï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
        "Referer": "https://movie.douban.com/top250",
        "Cookie": "bid=8JDp54oRCLY; viewed='1076013'; _vwo_uuid_v2=DC7A87659F426F9042AB34AE8532B7D56|ffba35a02b3d1fdeac99e182a2359c30; _pk_id.100001.4cf6=e2b081fa7b9a90be.1764294496.; __yadk_uid=cDHAB1ODmCX1uwAJJQqtOMLngl3RbVqB; push_noty_num=0; push_doumail_num=0; __utmz=30149280.1766056398.5.4.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); __utmz=223695111.1766056398.4.3.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); ll=\"118281\"; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1766326606%2C%22https%3A%2F%2Fwww.google.com.hk%2F%22%5D; _pk_ses.100001.4cf6=1; __utma=30149280.1598743586.1763973195.1766300945.1766326606.7; __utmc=30149280; __utma=223695111.382162774.1764294496.1766300945.1766326606.6; __utmb=223695111.0.10.1766326606; __utmc=223695111; frodotk_db=\"6053016a1c5285da20f9043e54f7918e\"; __utmv=30149280.29245; __utmb=30149280.20.10.1766326606; dbsawcv1=MTc2NjMzMTAwOUAxYzg2MDBiZDM4NzMyNmY1NGRkZGNiZjEyMmIxODIyM2ViNzkyMmZjNGUxZjk1NjgzNTdjMzBhOGNlMmM0Njc0QDk2YzdiNmI3OTNmZDUwODVANWUzYzZmZTM2ZWRj; dbcl2=\"292805490:ZcWWkZGovk8\"; ck=rOFD"
    }

    # åªçˆ¬å–å‰2é¡µï¼ˆå…±50éƒ¨ç”µå½±ï¼‰
    for start in [0, 25]:
        page = start // 25 + 1
        try:
            print(f"\n===== çˆ¬å–ç¬¬{page}é¡µ =====")
            # åˆ—è¡¨é¡µè¯·æ±‚å‰å¢åŠ å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººå·¥æµè§ˆ
            time.sleep(random.randint(3, 5))
            response = session.get(DOUBAN_TOP250_BASE.format(start), headers=headers, timeout=20)
            response.encoding = "utf-8"

            # åçˆ¬æ£€æµ‹
            if response.status_code != 200 or "æ£€æµ‹åˆ°å¼‚å¸¸æµé‡" in response.text:
                print(f"âš ï¸ ç¬¬{page}é¡µè§¦å‘åçˆ¬ï¼Œæš‚åœ15ç§’")
                time.sleep(15)
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            movie_items = soup.find_all("div", class_="item")
            if not movie_items:
                print("âš ï¸ è¯¥é¡µæ— ç”µå½±æ•°æ®")
                break

            # æ§åˆ¶çˆ¬å–æ•°é‡ä¸è¶…è¿‡50éƒ¨
            remaining = CRAWL_LIMIT - len(movies_data)
            for item in movie_items[:remaining]:
                # 1. ç”µå½±åç§°
                title_tag = item.find("span", class_="title")
                if not title_tag:
                    continue
                name = title_tag.text.strip()
                original_title = item.find("span", class_="original_title")
                if original_title and original_title.text.strip():
                    name += f" / {original_title.text.strip()}"

                # 2. ç”µå½±è¯¦æƒ…é¡µURLï¼ˆç”¨äºè·å–å®Œæ•´ç®€ä»‹ï¼‰
                detail_url = item.find("div", class_="hd").find("a")["href"]

                # 3. æµ·æŠ¥URL
                poster_img = item.find("img")
                poster_url = poster_img.get("src") or poster_img.get("data-origin", "")
                local_poster_url = download_poster(session, poster_url, name)

                # 4. å®Œæ•´ç®€ä»‹ï¼ˆä»è¯¦æƒ…é¡µè·å–ï¼‰
                intro = get_movie_intro(session, detail_url)

                # 5. è¯„åˆ†å’Œè¯„è®ºæ•°
                rating_tag = item.find("span", class_="rating_num")
                rating = float(rating_tag.text.strip()) if rating_tag else 0.0
                comment_span = item.find("span", text=lambda t: "äººè¯„ä»·" in t)
                comment_count = int(comment_span.text.replace("äººè¯„ä»·", "").replace(",", "")) if comment_span else 0

                # ç»„è£…æ•°æ®ï¼ˆå¯¹é½Movieæ¨¡å‹ï¼‰
                movies_data.append({
                    "name": name,
                    "poster_url": local_poster_url,
                    "intro": intro,
                    "initial_rating": rating,
                    "initial_comment_count": comment_count,
                    "category": "DoubanTop250",
                    "uploader_id": None
                })
            print(f"âœ… ç¬¬{page}é¡µå®Œæˆï¼Œç´¯è®¡{len(movies_data)}éƒ¨")

        except Exception as e:
            print(f"âŒ ç¬¬{page}é¡µå¤±è´¥ï¼š{str(e)}")
            continue
    return movies_data


def import_to_database(movies_data):
    """ä¿®å¤æ•°æ®åº“æ“ä½œï¼šåªåˆ é™¤è±†ç“£æ—§æ•°æ®ï¼Œé¿å…åˆ é™¤ç”¨æˆ·æ•°æ®"""
    with app.app_context():
        try:
            # åªåˆ é™¤è±†ç“£Top250çš„æ—§æ•°æ®
            old_count = Movie.query.filter_by(category="DoubanTop250").delete()
            print(f"â„¹ï¸ æ¸…ç©º{old_count}æ¡æ—§è±†ç“£æ•°æ®")

            # å»é‡ï¼ˆæŒ‰ç”µå½±åç§°ï¼‰
            unique_movies = {}
            for data in movies_data:
                if data["name"] not in unique_movies:
                    unique_movies[data["name"]] = data
            new_movies = [Movie(**data) for data in unique_movies.values()]

            if new_movies:
                db.session.add_all(new_movies)
                db.session.commit()
                print(f"ğŸ‰ æˆåŠŸå¯¼å…¥{len(new_movies)}éƒ¨è±†ç“£Top250ç”µå½±ï¼ˆå«å®Œæ•´ç®€ä»‹ï¼‰")
            else:
                print("â„¹ï¸ æ— æ–°ç”µå½±å¯å¯¼å…¥")
        except Exception as e:
            db.session.rollback()
            print(f"âŒ æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼š{str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    print("===== å¼€å§‹çˆ¬å–è±†ç“£Top50ç”µå½±ï¼ˆå«å®Œæ•´ç®€ä»‹ï¼‰ =====")
    # å…ˆåˆå§‹åŒ–é»˜è®¤æµ·æŠ¥ï¼ˆé¿å…ç©ºæ–‡ä»¶ï¼‰
    default_poster_path = os.path.join(POSTER_SAVE_PATH, "default.jpg")
    if not os.path.exists(default_poster_path):
        try:
            resp = requests.get("https://img2.doubanio.com/f/movie/8dd0c794499fe925ae2ae89ee30cd22575045749.jpg",
                                timeout=10)
            with open(default_poster_path, "wb") as f:
                f.write(resp.content)
            print("âœ… é»˜è®¤æµ·æŠ¥åˆå§‹åŒ–æˆåŠŸ")
        except:
            print("âš ï¸ é»˜è®¤æµ·æŠ¥ä¸‹è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºæ–‡ä»¶å…œåº•")
            with open(default_poster_path, "wb") as f:
                f.write(b"\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xd9")  # æœ€å°æœ‰æ•ˆJPG

    # çˆ¬å–+å¯¼å…¥
    movie_data_list = crawl_douban_top250()
    print(f"===== çˆ¬å–ç»“æŸï¼Œå…±{len(movie_data_list)}æ¡æ•°æ® =====")
    import_to_database(movie_data_list)
    print("===== æµç¨‹å®Œæˆ =====")